---
title: "Breast Cancer Prediction Report"
author: "Akansha Vashisth, Talha Siddiqui"
date: "2018/11/30"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = here::here())
```

```{r libraries, message=FALSE, include=FALSE}
library(tidyverse)
```

```{r read_data, message=FALSE, include=FALSE}
data = read_csv(here::here("results/detailed.csv"))

predictors <- data %>% select(-Classification, -Predictions, -Type)
```


## Introduction

Breast Cancer Predictions analysis aims to identify the strongest predictors of breast cancer. The question that it attempts to answer is as follows:

> What are the strongest predictors of breast cancer?

The analysis is built on the anthropometric data and parameters of 64 patients with breast cancer and 52 healthy controls collected by the Faculty of Medicine of the University of Coimbra and the University Hospital Centre of Coimbra. Identifying the strongest predictors of breast cancer in information that can be gathered in a routine blood analysis is hugely beneficial.

## Data Set

The data comprises of nine predictors, and a binary dependent variable indicating the presence or absence of breast cancer. All nine predictors are quantitative variables with positive values. 

### Assumptions

There are several underlying assumptions of the analysis. The first of these assumptions is reliability of the data. The analysis doesn't investigate the source and circumstances under which the data was collected. Additionally, the predictors available in the data are the only source of information available, meaning all else is assumed equal. Lastly, the model of choice for the analysis is assumed to be decision tree classification rather than any other supervised learning algorithm.

A summary of data set is as follows:

```{r data_summary, echo=FALSE}
knitr::kable(do.call(cbind, lapply(predictors, summary)), caption = 'Table 1. Summary of data set')
```

A visual distribution of each predictor separated by breast cancer patients and healthy controls is as follows:

```{r eda_plots, echo=FALSE, fig.width=10.5, fig.height=10.5, fig.cap='Figure 1. Distribution of predictors separated by classification'}
pngs = lapply(sprintf(here::here("img/plot%i.png"), 1:9), png::readPNG)
gl = lapply(pngs, grid::rasterGrob)
gridExtra::grid.arrange(grobs=gl, bottom = grid::textGrob("Figure 1. Distribution of predictors separated by classification",gp=grid::gpar(fontsize=12,font=1)))
```

In Figure 1 above, we observe bias in healthy controls for low values of Resistin, Glucose and Leptin. A few features, such as Insulin and HOMA, do not have bias between patients and healthy controls and most of the values are very low. There is a consistent spread of values in Age and BMI, but the values greatly overlap for both classifications.

## Analysis

The analysis to identify the strongest predictors is best addressed using a decision tree classification algorithm. This algorithm is parametric, which allows it to assess all the features and complete training data to pick the strongest predictors. Other supervised learning approaches that are non-parametric such as K-Nearest Neighbours would not be able to rank the predictors by their importance.

The configuration of the decision tree classifier algorithm was optimized for this analysis using 3-fold cross-validation. The limited size of data did not allow for greater number of folds. In this process, forty values from 1 to 40 for maximum tree depth and twenty values from 1 to 20 for minimum samples split were tested on a subset of the data and the optimum values were used to carry out the predictions. The accuracy of the predictions on training and test data are as follows:

```{r accuracy, echo=FALSE, fig.width=3, fig.height=4, fig.cap='Figure 2. Training and test accuracy'}
data %>% 
  mutate(Acc = Classification==Predictions) %>% 
  group_by(Type) %>% 
  summarise(Accuracy = sum(Acc)/n()) %>% 
  ggplot(aes(x = Type, y = Accuracy, fill = Accuracy)) + geom_col() + theme_bw() +
  theme(legend.position = "none") + labs(caption='Figure 2. Training and test accuracy')
```

The accuracy of predictions, as illustrated in the Figure 2, show low variance (measured as test error - train error) indicating very little overfitting in the model.

## Results

The result of the analysis indicates that Glucose, Leptin and Resistin are the strongest indicators of breast cancer. The complete list of parameters which are important in predicting breast cancer are as follows:

```{r result_image, cache=FALSE, fig.cap='Figure 3. Predictors of breast cancer', echo=FALSE}
knitr::include_graphics(path = "../results/results.png")
```

The analysis suggests that Glucose levels are around 57%, Leptin 25% and Resistin 18% indicative of breast cancer. The other predictors are not indicative of breast cancer. These results align with the observations in the exploratory data analysis above. The features that demonstrated bias were identified by the model as the strongest predictors of breast cancer.

## Critique

### Limitations

The primary limitation of the analysis is limited data. The model is trained on 80% of the total data which is only 92 examples. That is a very small sample for the model to accurately learn on. The model is then used to test on the remaining 20% of the data. The sizes of training and test data cast a doubt on the accuracy of the model. Additionally, the hyperparameters of the decision tree classifier, other than maximum depth and minimum samples split are not optimized but simply kept default.

Additionally, the simplistic anthropometric data and parameters that can be gathered in a routine blood analysis are certainly useful as biomarkers but are not sufficiently detailed to be key predictors.

## Future Direction

Although the breast cancer predictions analysis accomplishes the fundamental question posed in the beginning, there are several ways in which the analysis could be developed further. The ways in which this analysis can be advanced are as follows:

1.	Calculate likelihood of breast cancer in a new patient
2.	Compare and contrast feature importance derived by other parametric supervised learning models
3.	Conduct analysis on a bigger data set, such as [Kaggle's Breast Cancer Wisconsin Data Set](https://www.kaggle.com/hdza1991/breast-cancer-wisconsin-data-set).
